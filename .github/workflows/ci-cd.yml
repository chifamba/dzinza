name: Dzinza CI/CD Pipeline

# Workflow Execution Sequence:
# 1. Code Quality & Security (parallel): test, security, codeql
# 2. End-to-End Testing: e2e (after test, security, codeql)
# 3. Docker Build: docker (after test, security, codeql)
# 4. Staging Deployment: deploy-staging (after docker, for both develop & main branches)
# 5. Performance Testing: performance (after deploy-staging, acts as gate for production)
# 6. Production Deployment: deploy-production (after docker & performance, main branch only)
#
# Security scanning uses free open-source alternatives:
# - npm audit (built-in dependency vulnerability scanning)
# - Semgrep (static analysis security scanner)
# - CodeQL (GitHub's security analysis)
# - OSV-Scanner (Google's vulnerability scanner)
# - Trivy (comprehensive security scanner)
# - detect-secrets (credential exposure detection)a CI/CD Pipeline

# Workflow Execution Sequence:
# 1. Code Quality & Security (parallel): test, security
# 2. End-to-End Testing: e2e (after test, security)
# 3. Docker Build: docker (after test, security)
# 4. Staging Deployment: deploy-staging (after docker, for both develop & main branches)
# 5. Performance Testing: performance (after deploy-staging, acts as gate for production)
# 6. Production Deployment: deploy-production (after docker & performance, main branch only)
#
# This ensures performance testing validates the system before production deployment.

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  release:
    types: [published]

env:
  NODE_VERSION: "20"
  REGISTRY: ghcr.io
  # IMAGE_NAME is now set per service in the matrix strategy
  # Using github.repository_owner for the image name prefix for better organization context
  IMAGE_OWNER: ${{ github.repository_owner }}

jobs:
  # Code Quality and Testing
  test:
    name: Test and Quality Check
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"
          cache-dependency-path: |
            package-lock.json
            frontend/package-lock.json
            backend-service/package-lock.json

      - name: Install Dependencies (Robust)
        run: |
          # Install all workspace dependencies from root
          echo "Installing all workspace dependencies..."
          if [ -f "package-lock.json" ]; then
            npm ci
          else
            echo "Warning: Root package-lock.json not found, falling back to npm install"
            npm install
          fi
          
          # Install dependencies for individual services
          echo "Installing backend-service dependencies..."
          if [ -f "backend-service/package.json" ]; then
            cd backend-service
            if [ -f "package-lock.json" ]; then
              npm ci
            else
              echo "Installing backend-service dependencies with npm install"
              npm install
            fi
            cd ..
          fi
          
          echo "Installing frontend dependencies..."
          if [ -f "frontend/package.json" ]; then
            cd frontend
            if [ -f "package-lock.json" ]; then
              npm ci
            else
              echo "Installing frontend dependencies with npm install"
              npm install
            fi
            cd ..
          fi

      - name: Run Linting
        run: |
          # Try workspace command first, fallback to individual services
          if npm run lint 2>/dev/null; then
            echo "Ran linting via workspace command"
          else
            echo "Running linting for individual services..."
            if [ -f "frontend/package.json" ]; then
              cd frontend && npm run lint && cd ..
            fi
            if [ -f "backend-service/package.json" ] && cd backend-service && npm run | grep -q "lint"; then
              npm run lint && cd ..
            fi
          fi

      - name: Run Type Checking
        run: |
          # Try workspace command first, fallback to individual services
          if npm run typecheck 2>/dev/null; then
            echo "Ran typecheck via workspace command"
          else
            echo "Running typecheck for individual services..."
            if [ -f "frontend/package.json" ]; then
              cd frontend && npm run typecheck && cd ..
            fi
            if [ -f "backend-service/package.json" ] && cd backend-service && npm run | grep -q "typecheck"; then
              npm run typecheck && cd ..
            fi
          fi

      - name: Run Unit Tests
        run: |
          # Try workspace command first, fallback to individual services
          if npm run test:unit 2>/dev/null; then
            echo "Ran tests via workspace command"
          else
            echo "Running tests for individual services..."
            if [ -f "frontend/package.json" ]; then
              cd frontend && npm run test && cd ..
            fi
            if [ -f "backend-service/package.json" ] && cd backend-service && npm run | grep -q "test"; then
              npm run test && cd ..
            fi
          fi

      - name: Run Integration Tests
        run: |
          # Try workspace command first, fallback to individual services  
          if npm run test:integration 2>/dev/null; then
            echo "Ran integration tests via workspace command"
          else
            echo "Running integration tests for individual services..."
            if [ -f "frontend/package.json" ] && cd frontend && npm run | grep -q "test:integration"; then
              npm run test:integration && cd ..
            fi
            if [ -f "backend-service/package.json" ] && cd backend-service && npm run | grep -q "test:integration"; then
              npm run test:integration && cd ..
            fi
          fi

      - name: Upload Test Coverage
        uses: codecov/codecov-action@v3
        with:
          file: |
            ./frontend/coverage/lcov.info
            ./backend-service/coverage/lcov.info
          flags: unittests
          name: coverage-report

  # Security Scanning
  security:
    name: Security Scan
    runs-on: ubuntu-latest
    permissions:
      actions: read
      contents: read
      security-events: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"
          cache-dependency-path: |
            package-lock.json
            frontend/package-lock.json
            backend-service/package-lock.json

      - name: Install Dependencies (Robust)
        run: |
          # Install all workspace dependencies from root
          echo "Installing all workspace dependencies..."
          if [ -f "package-lock.json" ]; then
            npm ci
          else
            echo "Warning: Root package-lock.json not found, falling back to npm install"
            npm install
          fi
          
          # Install dependencies for individual services
          echo "Installing backend-service dependencies..."
          if [ -f "backend-service/package.json" ]; then
            cd backend-service
            if [ -f "package-lock.json" ]; then
              npm ci
            else
              echo "Installing backend-service dependencies with npm install"
              npm install
            fi
            cd ..
          fi
          
          echo "Installing frontend dependencies..."
          if [ -f "frontend/package.json" ]; then
            cd frontend
            if [ -f "package-lock.json" ]; then
              npm ci
            else
              echo "Installing frontend dependencies with npm install"
              npm install
            fi
            cd ..
          fi

      - name: Run Security Audit
        run: |
          # Try workspace command first, fallback to individual services
          if npm run security:audit 2>/dev/null; then
            echo "Ran security audit via workspace command"
          else
            echo "Running security audit for individual services..."
            if [ -f "frontend/package.json" ]; then
              echo "Auditing frontend..."
              cd frontend && npm audit --audit-level=moderate && cd ..
            fi
            if [ -f "backend-service/package.json" ]; then
              echo "Auditing backend-service..."
              cd backend-service && npm audit --audit-level=moderate && cd ..
            fi
          fi

      - name: Run Semgrep Security Scan
        uses: semgrep/semgrep-action@v1
        with:
          config: >-
            p/security-audit
            p/secrets
            p/ci
            p/owasp-top-ten
            p/javascript
            p/typescript
            p/react
            p/docker
        env:
          SEMGREP_APP_TOKEN: ${{ secrets.SEMGREP_APP_TOKEN }}

      - name: Run OSV-Scanner for Vulnerabilities
        uses: google/osv-scanner-action/osv-scanner-action@v2.0.2
        with:
          scan-args: |-
            --recursive
            --skip-git
            ./
        continue-on-error: true # Don't fail the pipeline for vulnerabilities, just report

      - name: Run Trivy Security Scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: "fs"
          scan-ref: "."
          format: "sarif"
          output: "trivy-results.sarif"
          exit-code: "0" # Don't fail pipeline
          severity: "CRITICAL,HIGH"

      - name: Check if SARIF file exists
        run: |
          if [ -f "trivy-results.sarif" ]; then
            echo "✅ SARIF file created successfully"
            ls -la trivy-results.sarif
          else
            echo "❌ SARIF file not found"
            ls -la
          fi

      - name: Upload Trivy SARIF results
        uses: github/codeql-action/upload-sarif@v3
        if: always() && hashFiles('trivy-results.sarif') != ''
        with:
          sarif_file: "trivy-results.sarif"
        continue-on-error: true # Don't fail if SARIF upload fails

      - name: Run Additional Security Checks
        run: |
          echo "Running additional open-source security checks..."

          # Check for exposed secrets using detect-secrets
          if ! command -v detect-secrets &> /dev/null; then
            pip install detect-secrets
          fi
          detect-secrets scan --all-files --baseline .secrets.baseline || true

          # Check for hardcoded credentials
          if command -v grep &> /dev/null; then
            echo "Checking for potential secrets..."
            grep -r -i "password\|secret\|key\|token" . --exclude-dir=node_modules --exclude-dir=.git --exclude="*.log" || true
          fi

  # CodeQL Security Analysis
  codeql:
    name: CodeQL Security Analysis
    runs-on: ubuntu-latest
    permissions:
      actions: read
      contents: read
      security-events: write
    strategy:
      fail-fast: false
      matrix:
        language: ["javascript", "typescript"]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Initialize CodeQL
        uses: github/codeql-action/init@v3
        with:
          languages: ${{ matrix.language }}
          queries: +security-and-quality

      - name: Setup Node.js for CodeQL
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"

      - name: Install Dependencies for Analysis
        run: |
          if [ -f "package.json" ]; then
            npm install || npm ci
          fi

      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v3
        with:
          category: "/language:${{matrix.language}}"

  # Build Application (REMOVED - Docker builds handle individual service builds)
  # build:
  #   name: Build Application
  #   runs-on: ubuntu-latest
  #   needs: [test, security]

  #   steps:
  #   - name: Checkout code
  #     uses: actions/checkout@v4

  #   - name: Setup Node.js
  #     uses: actions/setup-node@v4
  #     with:
  #       node-version: ${{ env.NODE_VERSION }}
  #       cache: 'npm'

  #   - name: Install dependencies
  #     run: npm ci # Installs all monorepo dependencies

  #   - name: Build application
  #     run: npm run build # This would typically build the frontend or specific workspace
  #     # If specific builds are needed for E2E outside Docker, E2E job should handle it.

  #   - name: Upload build artifacts
  #     uses: actions/upload-artifact@v4
  #     with:
  #       name: build-files # This artifact is no longer used by deploy jobs
  #       path: dist/ # Or specific service dist: e.g., packages/frontend/dist
  #       retention-days: 7

  # End-to-End Testing
  e2e:
    name: E2E Tests
    runs-on: ubuntu-latest
    # E2E tests might run against a deployed staging environment later, or use docker-compose to spin up services.
    # For now, it will run after 'test', 'security', and 'codeql'.
    needs: [test, security, codeql]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"
          cache-dependency-path: |
            frontend/package-lock.json

      - name: Install Frontend Dependencies for E2E
        working-directory: ./frontend
        run: |
          if [ -f "package-lock.json" ]; then
            npm ci
          else
            echo "Warning: package-lock.json not found, falling back to npm install"
            npm install
          fi

      # If E2E tests need to build a specific part of the application (e.g., frontend),
      # add a build step here: e.g., npm run build

      - name: Install Playwright
        working-directory: ./frontend
        run: npx playwright install --with-deps

      - name: Run E2E tests
        working-directory: ./frontend
        run: npm run test:e2e

      - name: Upload E2E test results
        uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: playwright-results
          path: frontend/test-results/
          retention-days: 7

  # Docker Build and Push (Multi-Service)
  docker:
    name: Build and Push Docker Image for ${{ matrix.service_name }}
    runs-on: ubuntu-latest
    needs: [test, security, codeql] # Removed 'build' dependency, added 'codeql' for comprehensive security
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop')
    permissions:
      contents: read
      packages: write
    strategy:
      fail-fast: false
      matrix:
        include:
          - service_name: auth-service
            dockerfile_path: ./auth-service/Dockerfile
          - service_name: backend-service
            dockerfile_path: ./backend-service/Dockerfile
          - service_name: gateway-service # Processed as a top-level service image
            dockerfile_path: ./backend-service/services/gateway/Dockerfile
          - service_name: frontend
            dockerfile_path: ./frontend/Dockerfile
          - service_name: genealogy-service
            dockerfile_path: ./genealogy-service/Dockerfile
          - service_name: search-service
            dockerfile_path: ./search-service/Dockerfile
          - service_name: storage-service
            dockerfile_path: ./storage-service/Dockerfile

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }} # github.actor is fine for personal repos, for orgs consider a specific service account or GITHUB_TOKEN permissions
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata for Docker
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_OWNER }}/${{ matrix.service_name }}
          tags: |
            type=sha,prefix=sha-,suffix=,format=short # e.g., sha-abcdef
            type=raw,value=latest,enable=${{ github.ref == 'refs/heads/main' }}
            # For develop branch, tag with 'develop' and a short SHA
            type=raw,value=develop-sha-${{ github.sha }},enable=${{ github.ref == 'refs/heads/develop' }}
            type=raw,value=develop,enable=${{ github.ref == 'refs/heads/develop' }}

      - name: Build and push Docker image for ${{ matrix.service_name }}
        id: build-and-push
        uses: docker/build-push-action@v6
        with:
          context: . # Monorepo root context
          file: ${{ matrix.dockerfile_path }}
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          # build-args: | # Example if build args were needed
          #   SERVICE_NAME=${{ matrix.service_name }}

  # Deploy to Staging
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: [docker] # Staging deployment depends on Docker images being available
    # Deploy to staging for both develop (for development testing) and main (for pre-production validation)
    if: github.ref == 'refs/heads/develop' || github.ref == 'refs/heads/main'
    environment: staging

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure kubectl for Staging
        run: |
          mkdir -p $HOME/.kube
          echo "${{ secrets.KUBE_CONFIG_DATA_STAGING }}" | base64 -d > $HOME/.kube/config
          chmod 600 $HOME/.kube/config
          kubectl version --client
        env:
          KUBECONFIG: $HOME/.kube/config

      - name: Database Migrations (Staging)
        run: |
          echo "Running database migrations for staging..."
          # This is a placeholder. Actual migration might involve:
          # 1. Ensure your Kubernetes cluster can access your GHCR images.
          # 2. Define a Kubernetes Job manifest (e.g., k8s/base/jobs/migrate-job.yaml).
          #    This job should use a service image that contains your migration scripts (e.g., auth-service or backend-service).
          #    The image for the job should be tagged with sha-${{ github.sha }} (short version).
          #
          # Example steps (conceptual):
          # Update migrate-job.yaml with the correct image:
          #   sed -i 's|image: .*\(service-with-migrations\):.*|image: ${{ env.REGISTRY }}/${{ env.IMAGE_OWNER }}/<service-with-migrations>:sha-${{ github.sha }}|g' k8s/base/jobs/migrate-job.yaml
          # Or use kustomize to set the image for the job.
          #
          # Apply the job:
          #   kubectl apply -f k8s/base/jobs/migrate-job.yaml -n staging
          # Wait for completion:
          #   kubectl wait --for=condition=complete job/migrate-db-staging --timeout=5m -n staging
          # Optional: Delete the job after completion
          #   kubectl delete job/migrate-db-staging -n staging
          echo "Migration step placeholder. Implement with K8s Job, correct image tagging, and namespace."

      - name: Deploy to Staging Environment
        run: |
          echo "Deploying to staging environment..."
          IMAGE_TAG_SUFFIX=$(echo $GITHUB_SHA | cut -c1-7) # Get short SHA
          IMAGE_TAG="sha-${IMAGE_TAG_SUFFIX}"
          echo "Using image tag: $IMAGE_TAG for k8s manifests if not using Kustomize to set images"

          # Option 1: Using Kustomize (Recommended)
          # Create a kustomization.yaml in k8s/overlays/staging that references base manifests.
          # Then, before applying, update images using kustomize edit set image:
          # echo "Updating images with Kustomize..."
          # cd k8s/overlays/staging
          # kustomize edit set image ghcr.io/OWNER_PLACEHOLDER/auth-service=${{ env.REGISTRY }}/${{ env.IMAGE_OWNER }}/auth-service:$IMAGE_TAG
          # kustomize edit set image ghcr.io/OWNER_PLACEHOLDER/backend-service=${{ env.REGISTRY }}/${{ env.IMAGE_OWNER }}/backend-service:$IMAGE_TAG
          # kustomize edit set image ghcr.io/OWNER_PLACEHOLDER/gateway-service=${{ env.REGISTRY }}/${{ env.IMAGE_OWNER }}/gateway-service:$IMAGE_TAG
          # kustomize edit set image ghcr.io/OWNER_PLACEHOLDER/frontend=${{ env.REGISTRY }}/${{ env.IMAGE_OWNER }}/frontend:$IMAGE_TAG
          # kustomize edit set image ghcr.io/OWNER_PLACEHOLDER/genealogy-service=${{ env.REGISTRY }}/${{ env.IMAGE_OWNER }}/genealogy-service:$IMAGE_TAG
          # kustomize edit set image ghcr.io/OWNER_PLACEHOLDER/search-service=${{ env.REGISTRY }}/${{ env.IMAGE_OWNER }}/search-service:$IMAGE_TAG
          # kustomize edit set image ghcr.io/OWNER_PLACEHOLDER/storage-service=${{ env.REGISTRY }}/${{ env.IMAGE_OWNER }}/storage-service:$IMAGE_TAG
          # kubectl apply -k . -n staging
          # cd ../../.. # Go back to repo root

          # Option 2: Plain kubectl apply (assumes manifests are in k8s/staging/ and image tags are manually updated or templated)
          # If using this, you MUST have a process to update image tags in your YAML files before applying.
          # For example, using `sed` or `envsubst` if your YAMLs are templates.
          # This example just applies all files in a directory - NOT PRODUCTION READY without image updates.
          echo "Applying raw manifests from k8s/staging/ (ensure images are correctly tagged in these files)..."
          if [ -d "k8s/staging" ]; then
            kubectl apply -R -f k8s/staging/ -n staging
          elif [ -d "k8s/overlays/staging" ]; then
            echo "Found k8s/overlays/staging. Consider using Kustomize for deployment."
            kubectl apply -k k8s/overlays/staging -n staging # This will work if kustomization.yaml sets images
          else
            echo "Error: No k8s/staging or k8s/overlays/staging directory found."
            exit 1
          fi
          echo "Deployment to staging initiated. Ensure your Kubernetes manifests reference the correct new image tags."

      - name: Run Staging Smoke Tests
        run: |
          echo "Running staging smoke tests..."
          # Example: Wait for deployment rollout and then curl health endpoints
          # kubectl rollout status deployment/auth-service -n staging --timeout=2m
          # AUTH_SERVICE_URL=$(kubectl get svc auth-service -n staging -o jsonpath='{.status.loadBalancer.ingress[0].ip}') # Or other ways to get URL/IP
          # if [ -z "$AUTH_SERVICE_URL" ]; then echo "Failed to get auth-service URL"; exit 1; fi
          # curl -sfL http://$AUTH_SERVICE_URL/health || exit 1
          # Repeat for other critical services
          echo "Smoke test placeholder. Implement actual health checks for deployed services."

      - name: Notify deployment status
        uses: 8398a7/action-slack@v3
        if: always()
        with:
          status: ${{ job.status }}
          channel: "#deployments"
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}

  # Deploy to Production
  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [docker, performance] # Production deployment depends on Docker images AND performance tests passing
    # Performance testing must pass before production deployment
    if: github.ref == 'refs/heads/main'
    environment: production

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure kubectl for Production
        run: |
          mkdir -p $HOME/.kube
          echo "${{ secrets.KUBE_CONFIG_DATA_PRODUCTION }}" | base64 -d > $HOME/.kube/config
          chmod 600 $HOME/.kube/config
          kubectl version --client
        env:
          KUBECONFIG: $HOME/.kube/config

      - name: Database Migrations (Production)
        run: |
          echo "Running database migrations for production..."
          # Similar to staging, implement with K8s Job or equivalent, targeting production DB.
          # Ensure the migration job uses the correct production-tagged service image.
          # Example:
          # kubectl apply -f k8s/base/jobs/migrate-job.yaml -n production
          # kubectl set image job/migrate-db-prod <container_name>=${{ env.REGISTRY }}/${{ env.IMAGE_OWNER }}/<service-with-migrations>:sha-${{ github.sha }} -n production
          # kubectl wait --for=condition=complete job/migrate-db-prod --timeout=5m -n production
          # kubectl delete job/migrate-db-prod -n production
          echo "Migration step placeholder. Implement with K8s Job, correct image tagging, and namespace."

      - name: Deploy to Production Environment
        run: |
          echo "Deploying to production environment..."
          IMAGE_TAG_SUFFIX=$(echo $GITHUB_SHA | cut -c1-7) # Get short SHA
          IMAGE_TAG="sha-${IMAGE_TAG_SUFFIX}" # This is the specific image version built. 'latest' also pushed for main.
          echo "Using image tag: $IMAGE_TAG for k8s manifests if not using Kustomize to set images (or 'latest' if appropriate for your strategy)"

          # Option 1: Using Kustomize (Recommended)
          # echo "Updating images with Kustomize for production..."
          # cd k8s/overlays/production
          # kustomize edit set image ghcr.io/OWNER_PLACEHOLDER/auth-service=${{ env.REGISTRY }}/${{ env.IMAGE_OWNER }}/auth-service:$IMAGE_TAG # Or :latest
          # ... for all services
          # kubectl apply -k . -n production
          # cd ../../..

          # Option 2: Plain kubectl apply (ensure images are updated in manifests)
          echo "Applying raw manifests from k8s/production/ (ensure images are correctly tagged in these files)..."
          if [ -d "k8s/production" ]; then
            kubectl apply -R -f k8s/production/ -n production
          elif [ -d "k8s/overlays/production" ]; then
            echo "Found k8s/overlays/production. Consider using Kustomize for deployment."
            kubectl apply -k k8s/overlays/production -n production # This will work if kustomization.yaml sets images
          else
            echo "Error: No k8s/production or k8s/overlays/production directory found."
            exit 1
          fi
          echo "Deployment to production initiated. Ensure your Kubernetes manifests reference the correct new image tags."

      - name: Run Production Smoke Tests
        run: |
          echo "Running production smoke tests..."
          # Similar to staging, implement actual health checks for deployed production services.
          echo "Smoke test placeholder."

      - name: Create GitHub release
        if: github.ref == 'refs/heads/main'
        uses: actions/create-release@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          tag_name: v${{ github.run_number }}
          release_name: Release v${{ github.run_number }}
          body: |
            Automated release created by GitHub Actions

            Changes in this release:
            ${{ github.event.head_commit.message }}
          draft: false
          prerelease: false

      - name: Notify deployment status
        uses: 8398a7/action-slack@v3
        if: always()
        with:
          status: ${{ job.status }}
          channel: "#deployments"
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}

  # Performance Testing
  performance:
    name: Performance Testing
    runs-on: ubuntu-latest
    needs: deploy-staging
    # Run performance tests for both develop (after staging deployment) and main (before production deployment)
    if: github.ref == 'refs/heads/develop' || github.ref == 'refs/heads/main'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Determine testing context
        run: |
          if [ "${{ github.ref }}" = "refs/heads/develop" ]; then
            echo "Running performance tests against staging environment for develop branch validation"
          elif [ "${{ github.ref }}" = "refs/heads/main" ]; then
            echo "Running performance tests against staging environment as gate before production deployment"
          fi

      - name: Setup k6
        uses: k6io/setup-k6@v0.2.0 # Or latest version
        with:
          k6-version: "latest" # Specify a k6 version or use 'latest'

      - name: Run Lighthouse CI # This can run in parallel or before/after k6 if desired
        uses: treosh/lighthouse-ci-action@v10
        with:
          configPath: "./lighthouserc.js" # Ensure this file exists if Lighthouse is used
          uploadArtifacts: true
          temporaryPublicStorage: true

      - name: Run k6 Auth Service Load Test
        run: |
          k6 run load-tests/auth-service-test.js \
            -e BASE_URL=${{ secrets.STAGING_AUTH_SERVICE_URL }} \
            --summary-export=auth-k6-summary.json \
            --out json=auth-k6-results.json
        continue-on-error: true # Allow pipeline to continue even if load tests have check failures, for reporting

      - name: Run k6 Genealogy Service Load Test
        run: |
          k6 run load-tests/genealogy-service-test.js \
            -e BASE_URL=${{ secrets.STAGING_GENEALOGY_SERVICE_URL }} \
            -e TREE_ID=${{ secrets.TEST_TREE_ID }} \
            -e PERSON_ID=${{ secrets.TEST_PERSON_ID }} \
            -e AUTH_TOKEN=${{ secrets.TEST_USER_AUTH_TOKEN }} \
            --summary-export=genealogy-k6-summary.json \
            --out json=genealogy-k6-results.json
        continue-on-error: true # Allow pipeline to continue for reporting

      - name: Upload k6 Test Results
        uses: actions/upload-artifact@v4
        if: always() # Upload results even if previous steps failed (e.g., k6 checks failed)
        with:
          name: k6-load-test-results
          path: |
            auth-k6-summary.json
            auth-k6-results.json
            genealogy-k6-summary.json
            genealogy-k6-results.json
          retention-days: 7
